{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from igraph import Graph\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from keras.layers import GlobalAveragePooling1D, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {}\n",
    "pairs_train = []\n",
    "pairs_test = []\n",
    "y_train = []\n",
    "with codecs.open('train.csv','r', 'UTF-8') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        if l[1] not in texts:\n",
    "            texts[l[1]] = l[3]\n",
    "        if l[2] not in texts:\n",
    "            texts[l[2]] = l[4]\n",
    "        pairs_train.append([l[1],l[2]])\n",
    "        y_train.append(int(l[5][:-1])) # [:-1] is just to remove formatting at the end\n",
    "\n",
    "with codecs.open('test.csv','r', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        if l[1] not in texts:\n",
    "            texts[l[1]] = l[3]\n",
    "        if l[2] not in texts:\n",
    "            texts[l[2]] = l[4][:-1]\n",
    "        pairs_test.append([l[1], l[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58940"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)  #type(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data: clean data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Getting Started on Quora: What is Quora?\"\n",
      "[315, 643, 19, 38, 2, 3, 38]\n",
      "(73, 'max_size')\n",
      "[315 643  19  38   2   3  38   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0]\n"
     ]
    }
   ],
   "source": [
    "docs = texts.values()\n",
    "# prepare tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "encoded_docs = tokenizer.texts_to_sequences(docs)\n",
    "print docs[0]\n",
    "print encoded_docs[0]\n",
    "#tokenizer.word_index\n",
    "\n",
    "# pad documents to the same length\n",
    "max_size = max([len(t) for t in encoded_docs]) # maximum document size allowed\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_size, padding='post')\n",
    "print (max_size, 'max_size')\n",
    "print padded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(t) for t in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(t) for t in encoded_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'getting', u'started', u'on', u'quora', u'what', u'is', u'quora']\n",
      "(599985, 'words')\n",
      "(20353, 'unique words')\n",
      "(20353, 'tokenizer.word_index')\n"
     ]
    }
   ],
   "source": [
    "index_to_word = dict((v,k) for k, v in tokenizer.word_index.items())\n",
    "# stpwds = [index_to_word[idx] for idx in range(1,stpwd_thd)]\n",
    "# print('stopwords are:',stpwds)\n",
    "x_full_words = [[index_to_word[idx] for idx in rev if idx!=0] for rev in encoded_docs]\n",
    "all_words = [word for rev in x_full_words for word in rev]\n",
    "print x_full_words[0]\n",
    "print (len(all_words),'words')\n",
    "print (len(list(set(all_words))),'unique words')\n",
    "print (len(tokenizer.word_index), 'tokenizer.word_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20353"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsFreq = Counter(all_words)   #print (wordsFreq)    #wordsFreq['the']\n",
    "len(wordsFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# texts_token is used for model fit\n",
    "texts_token = dict((ID,int(i)) for i,(ID,s) in enumerate(texts.items()))\n",
    "#padded_docs[texts_token[ID]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 'word_vector_dim')\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec embedding\n",
    "#Use pre-trained\n",
    "word_vector_dim = int(3e2)\n",
    "print (word_vector_dim, 'word_vector_dim')\n",
    "word_vectors = Word2Vec(size=word_vector_dim, min_count=1)\n",
    "#create entries for the words in our vocabulary\n",
    "word_vectors.build_vocab(x_full_words)\n",
    "#sanity check\n",
    "##assert(len(list(set(all_words))) == len(word_vectors.wv.vocab))\n",
    "word_vectors.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "json.dump(word_vectors, open(\"word_vectors.txt\",'w'))\n",
    "#word_vectors = json.load(open(\"word_vectors.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# of vocab words w/o a Google News entry:', 5351)\n"
     ]
    }
   ],
   "source": [
    "norms = [np.linalg.norm(word_vectors[word]) for word in list(word_vectors.wv.vocab)] # in Python 2.7: word_vectors.wv.vocab.keys()\n",
    "idxs_zero_norms = [idx for idx, norm in enumerate(norms) if norm<0.05]\n",
    "no_entry_words = [list(word_vectors.wv.vocab)[idx] for idx in idxs_zero_norms]\n",
    "print('# of vocab words w/o a Google News entry:',len(no_entry_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20353, 'unique words')\n",
      "(20353, 'word_vectors.wv.vocab')\n",
      "(20353, 'tokenizer.word_index')\n"
     ]
    }
   ],
   "source": [
    "print (len(list(set(all_words))),'unique words')\n",
    "print (len(word_vectors.wv.vocab), 'word_vectors.wv.vocab')\n",
    "print (len(tokenizer.word_index), 'tokenizer.word_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20354, 'max_features')\n",
      "embeddings created\n"
     ]
    }
   ],
   "source": [
    "# create numpy array of embeddings \n",
    "max_features = len(tokenizer.word_index)+1  # nb of unique words\n",
    "print (max_features, 'max_features')\n",
    "embeddings = np.zeros((max_features, word_vector_dim))\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    embeddings[idx,] = word_vectors[word]\n",
    "print('embeddings created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ..., \n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output embedding_matrix\n",
    "np.save('embedding_matrix.npy', embeddings)\n",
    "check = np.load('embedding_matrix.npy')\n",
    "check == embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---load embedding_matrix---#\n",
    "embeddings = np.load('embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Prepare training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_train = len(pairs_train)  ####padded_docs[texts_token[ID]]\n",
    "X_train1 = []\n",
    "X_train2 = []\n",
    "for i in range(len(pairs_train)):\n",
    "    q1 = pairs_train[i][0]\n",
    "    q2 = pairs_train[i][1]\n",
    "    X_train1 = X_train1 + [padded_docs[texts_token[q1]]]\n",
    "    X_train2 = X_train2 + [padded_docs[texts_token[q2]]]\n",
    "\n",
    "N_test = len(pairs_test)\n",
    "X_test1 = []\n",
    "X_test2 = []\n",
    "for i in range(len(pairs_test)):\n",
    "    q1 = pairs_test[i][0]\n",
    "    q2 = pairs_test[i][1]\n",
    "    X_test1 = X_test1 + [padded_docs[texts_token[q1]]]\n",
    "    X_test2 = X_test2 + [padded_docs[texts_token[q2]]]\n",
    "\n",
    "X_train1 = np.array(X_train1)\n",
    "X_train2 = np.array(X_train2)\n",
    "X_test1 = np.array(X_test1)\n",
    "X_test2 = np.array(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80100, 73) (80100, 73) 73\n",
      "(20179, 73) (20179, 73)\n"
     ]
    }
   ],
   "source": [
    "print X_train1.shape, X_train2.shape, max_size\n",
    "print X_test1.shape, X_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20179, 73)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([X_test1, X_test2]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_rebuilt = {}\n",
    "for id, words in texts.items():\n",
    "    s = x_full_words[texts_token[id]]\n",
    "    rebuilt = ''\n",
    "    for t in s:\n",
    "        rebuilt = rebuilt + t + ' '\n",
    "    texts_rebuilt[id] = rebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids2ind = {} # will contain the row idx of each unique text in the TFIDF matrix \n",
    "for qid in texts_rebuilt:\n",
    "    ids2ind[qid] = len(ids2ind)\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "A = vec.fit_transform(texts_rebuilt.values())\n",
    "\n",
    "basicF_train = np.zeros((N_train, 3))\n",
    "for i in range(len(pairs_train)):\n",
    "    q1 = pairs_train[i][0]\n",
    "    q2 = pairs_train[i][1]\n",
    "    basicF_train[i,0] = cosine_similarity(A[ids2ind[q1],:], A[ids2ind[q2],:])\n",
    "    basicF_train[i,1] = len(texts_rebuilt[q1].split()) + len(texts_rebuilt[q2].split())\n",
    "    basicF_train[i,2] = abs(len(texts_rebuilt[q1].split()) - len(texts_rebuilt[q2].split()))\n",
    "\n",
    "basicF_test = np.zeros((N_test, 3))\n",
    "for i in range(len(pairs_test)):\n",
    "    q1 = pairs_test[i][0]\n",
    "    q2 = pairs_test[i][1]\n",
    "    basicF_test[i,0] = cosine_similarity(A[ids2ind[q1],:], A[ids2ind[q2],:])\n",
    "    basicF_test[i,1] = len(texts_rebuilt[q1].split()) + len(texts_rebuilt[q2].split())\n",
    "    basicF_test[i,2] = abs(len(texts_rebuilt[q1].split()) - len(texts_rebuilt[q2].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.58723412,  22.        ,   6.        ],\n",
       "       [  0.44438048,  21.        ,   3.        ],\n",
       "       [  0.47480977,  26.        ,   6.        ],\n",
       "       ..., \n",
       "       [  0.62334832,  13.        ,   1.        ],\n",
       "       [  0.71035487,  38.        ,   2.        ],\n",
       "       [  0.6320477 ,  18.        ,   0.        ]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicF_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the basic features\n",
    "with open(\"basic_features_train.csv\", 'w') as f:\n",
    "    f.write(\"tfidfCOS, lenSUM, lenDIFF\\n\")\n",
    "    for i in range(N_train):\n",
    "        f.write(str(basicF_train[i,0])+','+str(basicF_train[i,1])+','+str(basicF_train[i,2])+'\\n')\n",
    "\n",
    "with open(\"basic_features_test.csv\", 'w') as f:\n",
    "    f.write(\"tfidfCOS, lenSUM, lenDIFF\\n\")\n",
    "    for i in range(N_test):\n",
    "        f.write(str(basicF_test[i,0])+','+str(basicF_test[i,1])+','+str(basicF_test[i,2])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80100, 3) (20179, 3)\n"
     ]
    }
   ],
   "source": [
    "#---load basic features---#\n",
    "basicF_train = pd.read_csv('basic_features_train.csv')\n",
    "basicF_train = basicF_train.values\n",
    "basicF_test = pd.read_csv('basic_features_test.csv')\n",
    "basicF_test = basicF_test.values\n",
    "print basicF_train.shape, basicF_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic feature and Graph feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import itertools\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terms2graph(terms, w, vocab, d):\n",
    "    '''This function use \n",
    "       returns an undirected, unweighted graph \n",
    "    '''    \n",
    "    #(1) Terms to graph\n",
    "    terms = [term for term in terms if term]   #remove the padding zero\n",
    "        \n",
    "    v_nb = len(vocab)\n",
    "    A = np.zeros((v_nb,v_nb))  #adjacency matrix\n",
    "    D = np.zeros((v_nb,v_nb))\n",
    "    for term in terms:\n",
    "        D[vocab[term],vocab[term]] = 1\n",
    "    # create initial complete graph (first w terms)\n",
    "    \n",
    "    terms_temp = terms[0:w]\n",
    "    \n",
    "    if len(terms_temp)>=2:\n",
    "        indexes = list(itertools.combinations(range(w), r=2))\n",
    "    else:\n",
    "        indexes = []\n",
    "    \n",
    "    # add edges\n",
    "    edges = []\n",
    "    for my_tuple in indexes:\n",
    "        ii = vocab[terms_temp[my_tuple[0]]]    # index of vertice\n",
    "        jj = vocab[terms_temp[my_tuple[1]]]\n",
    "        if ii != jj:\n",
    "            A[ii, jj] = 1\n",
    "            A[jj, ii] = 1\n",
    "            edges.append((ii,jj))\n",
    "    # then iterate over the remaining terms\n",
    "    for i in range(w, len(terms)):\n",
    "        considered_term = terms[i] # term to consider\n",
    "        terms_temp = terms[(i-w+1):(i+1)] # all terms within sliding window\n",
    "        \n",
    "        # edges to try\n",
    "        candidate_edges = []\n",
    "        for p in range(w-1):\n",
    "            candidate_edges.append((terms_temp[p],considered_term))\n",
    "        \n",
    "        for try_edge in candidate_edges:          \n",
    "            if try_edge[1] != try_edge[0]:\n",
    "            # if not self-edge\n",
    "                ii = vocab[try_edge[0]]\n",
    "                jj = vocab[try_edge[1]]\n",
    "                A[ii, jj] = 1\n",
    "                A[jj, ii] = 1\n",
    "                edges.append((ii,jj))\n",
    "    \n",
    "    #print A, np.sum(A)\n",
    "    #print D\n",
    "    \n",
    "    # create empty graph\n",
    "    g = nx.Graph()\n",
    "    # add vertices\n",
    "    g.add_nodes_from(sorted(vocab.values()))\n",
    "    g.add_edges_from(edges)\n",
    "    \n",
    "    #(2) Convert the graph using shortest_path_length\n",
    "    AA = np.zeros((v_nb,v_nb))\n",
    "    path = nx.shortest_path_length(g)\n",
    "    \n",
    "    for v1 in range(len(path)):\n",
    "        if len(path[v1])>1:   #len(path[0])=1 means this vertice is not in this graph\n",
    "            path_sorted = sorted(path[v1].items(), key=operator.itemgetter(1)) # dictionary sorted by value\n",
    "            for v2, l in path_sorted[1:]:\n",
    "                if l <= d:\n",
    "                    AA[v1,v2] = 1.0/l\n",
    "    \n",
    "    M = AA + D\n",
    "    norm = np.linalg.norm(M)                  \n",
    "    #print AA    \n",
    "    return(g, AA, norm)\n",
    "\n",
    "def graph_kernel(x1, x2, A1, A2, norm):\n",
    "    k_node = len(set(x1).intersection(set(x2)).difference({0}))\n",
    "    k_walk = np.sum(A1 * A2)  \n",
    "    #print k_node, k_walk\n",
    "    k = (k_node + k_walk)/norm    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_similarity_train = []\n",
    "for i in range(N_train):\n",
    "    # vocabulary for X_train1[i] and X_train2[i]\n",
    "    vocab_per_train = sorted(list(set(X_train1[i]).union(set(X_train2[i])).difference({0})))\n",
    "    vocab_per_train = dict((word,index) for index,word in enumerate(vocab_per_train))\n",
    "    w = 2  #window\n",
    "    d = 3  #walk length\n",
    "    g1,A1,norm1 = terms2graph(X_train1[i], w, vocab_per_train, d)\n",
    "    g2,A2,norm2 = terms2graph(X_train2[i], w, vocab_per_train, d)\n",
    "    norm = norm1 * norm2\n",
    "    graph_similarity_train.append(graph_kernel(X_train1[i],X_train2[i],A1,A2,norm))\n",
    "graph_similarity_train = np.array(graph_similarity_train).reshape(-1,1)\n",
    "\n",
    "graph_similarity_test = []\n",
    "for i in range(N_test):\n",
    "    # vocabulary for X_test1[i] and X_test2[i]\n",
    "    vocab_per_test = sorted(list(set(X_test1[i]).union(set(X_test2[i])).difference({0})))\n",
    "    vocab_per_test = dict((word,index) for index,word in enumerate(vocab_per_test))\n",
    "    w = 2  #window\n",
    "    d = 3  #walk length\n",
    "    g1,A1,norm1 = terms2graph(X_test1[i], w, vocab_per_test, d)\n",
    "    g2,A2,norm2 = terms2graph(X_test2[i], w, vocab_per_test, d)\n",
    "    norm = norm1 * norm2\n",
    "    graph_similarity_test.append(graph_kernel(X_test1[i],X_test2[i],A1,A2,norm))\n",
    "graph_similarity_test = np.array(graph_similarity_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###??? deal with NaN data\n",
    "graph_similarity_train = np.nan_to_num(graph_similarity_train)\n",
    "graph_similarity_test = np.nan_to_num(graph_similarity_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80100, 1), (20179, 1))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_similarity_train.shape, graph_similarity_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0000000000000004)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(graph_similarity_train), np.max(graph_similarity_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26013299],\n",
       "       [ 0.76271277],\n",
       "       [ 0.72488244],\n",
       "       ..., \n",
       "       [ 0.81016272],\n",
       "       [ 0.09058216],\n",
       "       [ 0.52574973]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d=1\n",
    "graph_similarity_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24781018],\n",
       "       [ 0.8046798 ],\n",
       "       [ 0.74926865],\n",
       "       ..., \n",
       "       [ 0.8054638 ],\n",
       "       [ 0.07470439],\n",
       "       [ 0.50619689]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d=2\n",
    "graph_similarity_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24525303],\n",
       "       [ 0.81160375],\n",
       "       [ 0.75241203],\n",
       "       ..., \n",
       "       [ 0.80217473],\n",
       "       [ 0.06880674],\n",
       "       [ 0.50235602]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d=3\n",
    "graph_similarity_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the graph features\n",
    "with open(\"graph_features_train.csv\", 'w') as f:\n",
    "    f.write(\"graphKernel\\n\")\n",
    "    for i in range(N_train):\n",
    "        f.write(str(graph_similarity_train[i,0])+'\\n')\n",
    "\n",
    "with open(\"graph_features_test.csv\", 'w') as f:\n",
    "    f.write(\"graphKernel\\n\")\n",
    "    for i in range(N_test):\n",
    "        f.write(str(graph_similarity_test[i,0])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---load graph features---#\n",
    "graph_similarity_train = pd.read_csv('graph_features_train.csv')\n",
    "graph_similarity_train = graph_similarity_train.values\n",
    "graph_similarity_test = pd.read_csv('graph_features_test.csv')\n",
    "graph_similarity_test = graph_similarity_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26013299,  0.24781018,  0.24525303],\n",
       "       [ 0.76271277,  0.8046798 ,  0.81160376],\n",
       "       [ 0.72488244,  0.74926865,  0.75241203],\n",
       "       ..., \n",
       "       [ 0.81016272,  0.8054638 ,  0.80217472],\n",
       "       [ 0.09058216,  0.07470439,  0.06880674],\n",
       "       [ 0.52574973,  0.50619689,  0.50235602]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_similarity_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit learn ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 28161, 1: 51939})"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51418056795360845"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use graph feature to predict\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "trainSize = int(N_train * 0.9)\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=10, n_jobs=-1)\n",
    "#clf = AdaBoostClassifier(n_estimators=500)\n",
    "#clf = svm.SVC(probability=True)\n",
    "#clf = MLPClassifier()\n",
    "\n",
    "#features_train = basicF_train\n",
    "#features_test = basicF_test\n",
    "features_train = np.concatenate((basicF_train, graph_similarity_train), axis=1)\n",
    "features_test = np.concatenate((basicF_test, graph_similarity_test), axis=1)\n",
    "\n",
    "clf.fit(features_train[:trainSize,:], y_train[:trainSize])\n",
    "y_pred = clf.predict_proba(features_train[trainSize:,:])\n",
    "#y_true = [0, 0, 1, 1]\n",
    "#y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n",
    "y_true = y_train[trainSize:]\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.51508038460919536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict test data, then output to a CSV file\n",
    "y_pred_test = clf.predict_proba(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        f.write(str(i)+','+str(y_pred_test[i][1])+'\\n')   #y_pred[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 , loss 0.514883539309\n",
      "11 , loss 0.513655911954\n",
      "12 , loss 0.512710427586\n",
      "13 , loss 0.513062037218\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-243-eadc3a2bf90e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrainSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrainSize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrainSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrainSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 328\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda2\\lib\\multiprocessing\\pool.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda2\\lib\\multiprocessing\\pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda2\\lib\\threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10,20):\n",
    "    clf = RandomForestClassifier(n_estimators=500, max_depth=i, n_jobs=-1)\n",
    "    clf.fit(features_train[:trainSize,:], y_train[:trainSize])\n",
    "    y_pred = clf.predict_proba(features_train[trainSize:,:])\n",
    "    y_true = y_train[trainSize:]\n",
    "    print i, ', loss', log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------VotingClassifier---------------\n",
      "\n",
      "[GradientBoosting] Training: 0.5212\n",
      "[GradientBoosting] Test:     0.5246 \n",
      "\n",
      "[MLP] Training: 0.5373\n",
      "[MLP] Test:     0.5398 \n",
      "\n",
      "[RandomForest] Training: 0.4762\n",
      "[RandomForest] Test:     0.5128 \n",
      "\n",
      "[VotingClassifier] Training: 0.5060\n",
      "[VotingClassifier] Test:     0.5206 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "# create the sub models\n",
    "estimators = []\n",
    "#model1 = KNeighborsClassifier(n_neighbors=8)\n",
    "model1 = GradientBoostingClassifier(n_estimators=100,random_state=0)\n",
    "estimators.append(('GradientBoosting', model1))\n",
    "#extra = ExtraTreesClassifier(n_estimators=35, random_state=1)\n",
    "#model2 = AdaBoostClassifier(base_estimator=extra, n_estimators=100, random_state=0)\n",
    "model2 = MLPClassifier()\n",
    "estimators.append(('MLP', model2))\n",
    "model3 = RandomForestClassifier(n_estimators = 500, max_depth=12, random_state=0)\n",
    "estimators.append(('RandomForest', model3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators,voting='soft')\n",
    "###ensemble = ensemble.fit(X_train, y_train)\n",
    "\n",
    "print('---------------VotingClassifier---------------')\n",
    "      \n",
    "for clf, label in zip([model1, model2, model3, ensemble], \n",
    "                      ['GradientBoosting', \n",
    "                       'MLP', \n",
    "                       'RandomForest',\n",
    "                       'VotingClassifier']):\n",
    "\n",
    "    #scores = cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    #print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    clf = clf.fit(features_train[:trainSize,:], y_train[:trainSize])\n",
    "    \n",
    "    y_pred = clf.predict_proba(features_train[:trainSize,:])\n",
    "    y_true = y_train[:trainSize]\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    print(\"\\n[%s] Training: %0.4f\" % (label, loss))\n",
    "\n",
    "    y_pred = clf.predict_proba(features_train[trainSize:,:])\n",
    "    y_true = y_train[trainSize:]\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    print(\"[%s] Test:     %0.4f \" % (label, loss))\n",
    "\n",
    "clf = ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------StackingClassifier---------------\n",
      "\n",
      "[GradientBoost] Training: 0.5212\n",
      "[GradientBoost] Test:     0.5246 \n",
      "\n",
      "[MLP] Training: 0.5414\n",
      "[MLP] Test:     0.5451 \n",
      "\n",
      "[RandomForest] Training: 0.4762\n",
      "[RandomForest] Test:     0.5128 \n",
      "\n",
      "[StackingClassifier] Training: 0.5034\n",
      "[StackingClassifier] Test:     0.5589 \n"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "clf1 = GradientBoostingClassifier(n_estimators=100,random_state=1)\n",
    "clf2 = MLPClassifier()\n",
    "#tree = RandomForestClassifier(n_estimators = 500, max_features=None, random_state=0)\n",
    "#extra = ExtraTreesClassifier(n_estimators=35, random_state=1)\n",
    "#clf2 = AdaBoostClassifier(base_estimator=extra, n_estimators=100, random_state=0)\n",
    "#clf2 = Perceptron()\n",
    "clf3 = RandomForestClassifier(n_estimators = 500, max_depth=12, random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr)\n",
    "#sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], use_probas=True, average_probas=False, meta_classifier=lr)\n",
    "\n",
    "print('---------------StackingClassifier---------------')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
    "                      ['GradientBoost', \n",
    "                       'MLP', \n",
    "                       'RandomForest',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    #scores = cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    #print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" % (scores.mean(), scores.std(), label))    \n",
    "    clf = clf.fit(features_train[:trainSize,:], y_train[:trainSize])\n",
    "    \n",
    "    y_pred = clf.predict_proba(features_train[:trainSize,:])\n",
    "    y_true = y_train[:trainSize]\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    print(\"\\n[%s] Training: %0.4f\" % (label, loss))\n",
    "\n",
    "    y_pred = clf.predict_proba(features_train[trainSize:,:])\n",
    "    y_true = y_train[trainSize:]\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    print(\"[%s] Test:     %0.4f \" % (label, loss))\n",
    "\n",
    "clf = sclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict test data, then output to a CSV file\n",
    "y_pred_test = clf.predict_proba(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        f.write(str(i)+','+str(y_pred_test[i][1])+'\\n')   #y_pred[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2,\n",
       "         1: 10,\n",
       "         2: 5,\n",
       "         3: 230,\n",
       "         4: 1348,\n",
       "         5: 2862,\n",
       "         6: 4833,\n",
       "         7: 7035,\n",
       "         8: 7788,\n",
       "         9: 7353,\n",
       "         10: 6249,\n",
       "         11: 4535,\n",
       "         12: 3479,\n",
       "         13: 2618,\n",
       "         14: 2392,\n",
       "         15: 2121,\n",
       "         16: 1544,\n",
       "         17: 924,\n",
       "         18: 774,\n",
       "         19: 630,\n",
       "         20: 437,\n",
       "         21: 284,\n",
       "         22: 333,\n",
       "         23: 204,\n",
       "         24: 178,\n",
       "         25: 182,\n",
       "         26: 128,\n",
       "         27: 103,\n",
       "         28: 109,\n",
       "         29: 48,\n",
       "         30: 42,\n",
       "         31: 37,\n",
       "         32: 24,\n",
       "         33: 20,\n",
       "         34: 11,\n",
       "         35: 15,\n",
       "         36: 9,\n",
       "         37: 2,\n",
       "         38: 10,\n",
       "         39: 7,\n",
       "         40: 3,\n",
       "         41: 3,\n",
       "         42: 3,\n",
       "         43: 3,\n",
       "         45: 1,\n",
       "         46: 3,\n",
       "         49: 2,\n",
       "         52: 1,\n",
       "         53: 1,\n",
       "         54: 1,\n",
       "         58: 1,\n",
       "         59: 1,\n",
       "         65: 1,\n",
       "         73: 1})"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_len = [len(t) for t in encoded_docs]\n",
    "Counter(document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yaya = sorted(list(set(X_train1[7265]).union(set(X_train2[7265])).difference({0})))\n",
    "yaya = dict((word,index) for index,word in enumerate(yaya))\n",
    "w = 2\n",
    "d = 2\n",
    "#g1 = terms_to_graph(X_train1[0], w, yaya)\n",
    "g1,A1,norm1 = terms2graph(X_train1[7265], w, yaya, d)\n",
    "g2,A2,norm2 = terms2graph(X_train2[7265], w, yaya, d)\n",
    "norm = norm1 * norm2\n",
    "graph_similarity = graph_kernel(X_train1[7265],X_train2[7265],A1,A2,norm)\n",
    "graph_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fea_names = []\n",
    "#fea_names.append('q1_freq')   \n",
    "#graph_train = np.array(graph_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fea_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['q1_freq', 'q2_freq'], \n",
       "      dtype='|S7')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(fea_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1_freq</th>\n",
       "      <th>q2_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q1_freq  q2_freq\n",
       "0        1        2\n",
       "1        3        4\n",
       "2        5        6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output graph features to a CSV file\n",
    "graph_train = np.array(graph_train)\n",
    "features = pd.DataFrame(graph_train, columns=fea_names)\n",
    "features.to_csv(\"feature_Graph_train.csv\", index=False)\n",
    "features.head()\n",
    "#graph_train = pd.read_csv('feature_Graph_train.csv')\n",
    "#fea_names = graph_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deep Learning CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_conv1D_(emb_matrix):\n",
    "    \n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=max_size,\n",
    "        trainable=False\n",
    "    )\n",
    "    print (emb_matrix.shape, 'emb_matrix shape')\n",
    "    \n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(max_size,))    #max_size: maximum document size allowed\n",
    "    seq2 = Input(shape=(max_size,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    conv1b = conv1(emb2)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "    conv2b = conv2(emb2)\n",
    "    glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    conv3b = conv3(emb2)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "    conv4b = conv4(emb2)\n",
    "    glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    conv5b = conv5(emb2)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "    conv6b = conv6(emb2)\n",
    "    glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "    #print (glob1a.shape,glob2a.shape,glob5a.shape)\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "    # We take the explicit absolute difference between the two sentences\n",
    "    # Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    \n",
    "    # Add basic features\n",
    "    basic_shape = basicF_train.shape[1]\n",
    "    basic_input = Input(shape=(basic_shape,))\n",
    "    basic_dense = BatchNormalization()(basic_input)\n",
    "    basic_dense = Dense(64, activation='relu')(basic_dense)\n",
    "    \n",
    "    # Add graph feature\n",
    "    graph_shape = graph_similarity_train.shape[1]\n",
    "    graph_input = Input(shape=(graph_shape,))\n",
    "    graph_dense = BatchNormalization()(graph_input)\n",
    "    graph_dense = Dense(64, activation='relu')(graph_dense)    \n",
    "    \n",
    "    # Add the magic features\n",
    "    ###magic_input = Input(shape=(5,))\n",
    "    ###magic_dense = BatchNormalization()(magic_input)\n",
    "    ###magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "    # Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "    # nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "    #distance_input = Input(shape=(20,))\n",
    "    #distance_dense = BatchNormalization()(distance_input)\n",
    "    #distance_dense = Dense(128, activation='relu')(distance_dense)\n",
    "\n",
    "    # Merge the Magic and distance features with the difference layer\n",
    "    #merge = concatenate([diff, mul, magic_dense, distance_dense])\n",
    "    #merge = concatenate([diff, mul, magic_dense])\n",
    "    #merge = concatenate([diff, mul])\n",
    "    merge = concatenate([diff, mul, basic_dense, graph_dense])\n",
    "\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    #model = Model(inputs=[seq1, seq2], outputs=pred)\n",
    "    model = Model(inputs=[seq1, seq2, basic_input, graph_input], outputs=pred)\n",
    "    #model = Model(inputs=[seq1, seq2, magic_input], outputs=pred)\n",
    "    #model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26432 samples, validate on 53668 samples\n",
      "Epoch 1/3\n",
      "  256/26432 [..............................] - ETA: 52:53 - loss: 0.8705 - acc: 0.4727"
     ]
    }
   ],
   "source": [
    "X_train = [np.array(X_train1),np.array(X_train2)]\n",
    "model = model_conv1D_(embeddings)\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size = 128,\n",
    "          epochs = 3,\n",
    "          verbose = 1,\n",
    "          validation_split = 0.1)\n",
    "          #, class_weight={0: class_weights[0], 1: class_weights[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80100, 73), (80100, 3), (80100, 1))"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape,basicF_train.shape, graph_similarity_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((20354, 300), 'emb_matrix shape')\n"
     ]
    }
   ],
   "source": [
    "#X_train = [np.array(X_train1),np.array(X_train2)]\n",
    "model = model_conv1D_(embeddings)\n",
    "model.fit([X_train1, X_train2, basicF_train, graph_similarity_train],\n",
    "          y_train,\n",
    "          batch_size = 128,\n",
    "          epochs = 3,\n",
    "          verbose = 1,\n",
    "          validation_split = 0.1)\n",
    "          #, class_weight={0: class_weights[0], 1: class_weights[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output prediction of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = [np.array(X_test1),np.array(X_test2)]\n",
    "y_pred = model.predict(X_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        f.write(str(i)+','+str(y_pred[i][0])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delete...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80100, 53667, 26433)\n",
      "(80100, 53667, 26433)\n",
      "(80100, 53667, 26433)\n"
     ]
    }
   ],
   "source": [
    "# Split training data\n",
    "train_size = int(N_train * 0.67)\n",
    "X_trainTrain1 = X_train1[:train_size]\n",
    "X_trainTest1 = X_train1[train_size:]\n",
    "X_trainTrain2 = X_train2[:train_size]\n",
    "X_trainTest2 = X_train2[train_size:]\n",
    "\n",
    "y_trainTrain = y_train[:train_size]\n",
    "y_trainTest = y_train[train_size:]\n",
    "print (len(X_train1), len(X_trainTrain1), len(X_trainTest1))\n",
    "print (len(X_train2), len(X_trainTrain2), len(X_trainTest2))\n",
    "print (len(y_train), len(y_trainTrain), len(y_trainTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit([np.array(X_trainTrain1),np.array(X_trainTrain2)],\n",
    "          np.array(y_trainTrain),\n",
    "          batch_size = 128,\n",
    "          epochs = 3,\n",
    "          verbose = 1,\n",
    "          validation_data = ([np.array(X_trainTest1),np.array(X_trainTest2)], np.array(y_trainTest)))\n",
    "          #, class_weight={0: 1, 1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids2ind = {} # will contain the row idx of each unique text in the TFIDF matrix \n",
    "for qid in texts:\n",
    "    ids2ind[qid] = len(ids2ind)\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "A = vec.fit_transform(texts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_train = len(pairs_train)\n",
    "X_train = np.zeros((N_train, 3))\n",
    "for i in range(len(pairs_train)):\n",
    "    q1 = pairs_train[i][0]\n",
    "    q2 = pairs_train[i][1]\n",
    "    X_train[i,0] = cosine_similarity(A[ids2ind[q1],:], A[ids2ind[q2],:])\n",
    "    X_train[i,1] = len(texts[q1].split()) + len(texts[q2].split())\n",
    "    X_train[i,2] = abs(len(texts[q1].split()) - len(texts[q2].split()))\n",
    "\n",
    "N_test = len(pairs_test)\n",
    "X_test = np.zeros((N_test, 3))\n",
    "for i in range(len(pairs_test)):\n",
    "    q1 = pairs_test[i][0]\n",
    "    q2 = pairs_test[i][1]\n",
    "    X_test[i,0] = cosine_similarity(A[ids2ind[q1],:], A[ids2ind[q2],:])\n",
    "    X_test[i,1] = len(texts[q1].split()) + len(texts[q2].split())\n",
    "    X_test[i,2] = abs(len(texts[q1].split()) - len(texts[q2].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=3, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80100, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_train = len(pairs_train)\n",
    "X_train1 = np.zeros((N_train, 4))\n",
    "X_train1[:,:3] = X_train\n",
    "for i in range(len(pairs_train)):\n",
    "    q1 = pairs_train[i][0]\n",
    "    q2 = pairs_train[i][1]\n",
    "    X_train1[i,3] = symmetric_sentence_similarity(texts[q1], texts[q2])\n",
    "\n",
    "N_test = len(pairs_test)\n",
    "X_test1 = np.zeros((N_test, 4))\n",
    "X_test1[:,:3] = X_test\n",
    "for i in range(len(pairs_test)):\n",
    "    q1 = pairs_test[i][0]\n",
    "    q2 = pairs_test[i][1]\n",
    "    X_test1[i,3] = symmetric_sentence_similarity(texts[q1], texts[q2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(X_train1[:,3])):\n",
    "    if X_train1[i,3]<0:\n",
    "        X_train1[i,3] = X_train1[i,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_train = len(pairs_train)\n",
    "X_train2 = np.zeros((N_train, 5))\n",
    "X_train2[:,:4] = X_train1\n",
    "for i in range(len(pairs_train)):\n",
    "    q1 = pairs_train[i][0]\n",
    "    q2 = pairs_train[i][1]\n",
    "    X_train2[i,4] = similarity(texts[q1], texts[q2], True)\n",
    "\n",
    "N_test = len(pairs_test)\n",
    "X_test2 = np.zeros((N_test, 5))\n",
    "X_test2[:,:4] = X_test1\n",
    "for i in range(len(pairs_test)):\n",
    "    q1 = pairs_test[i][0]\n",
    "    q2 = pairs_test[i][1]\n",
    "    X_test2[i,4] = similarity(texts[q1], texts[q2], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 4), (0, 4), (0, 4))\n",
      "(80100, 53667, 26433)\n"
     ]
    }
   ],
   "source": [
    "# Split training data\n",
    "N_train = len(pairs_train)\n",
    "train_size = int(N_train * 0.67)\n",
    "X_trainTrain = X_train[:train_size, :]\n",
    "X_trainTest = X_train[train_size:, :]\n",
    "\n",
    "y_trainTrain = y_train[:train_size]\n",
    "y_trainTest = y_train[train_size:]\n",
    "print (X_train1.shape, X_trainTrain.shape, X_trainTest.shape)\n",
    "print (len(y_train), len(y_trainTrain), len(y_trainTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = MLPClassifier()\n",
    "#clf1 = RandomForestClassifier(n_estimators=500, max_depth=3, n_jobs=-1)\n",
    "clf1.fit(X_trainTrain, y_trainTrain)   # X_trainTrain[:,(0,3)]\n",
    "y_pred = clf1.predict_proba(X_trainTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55977646975724693"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "#y_true = [0, 0, 1, 1]\n",
    "#y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n",
    "y_true = y_trainTest\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction of the test data, then output to a CSV file\n",
    "clf1 = MLPClassifier()\n",
    "clf1.fit(X_train1, y_train)\n",
    "y_pred = clf1.predict_proba(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        f.write(str(i)+','+str(y_pred[i][1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terms_to_graph(terms, w, vocab):\n",
    "    '''This function returns an undirected, unweighted graph \n",
    "    from a list of terms (the tokens from the pre-processed text) \n",
    "    e.g., ['quick','brown','fox'].\n",
    "    Edges are unweighted representing term co-occurence \n",
    "    within a sliding window of fixed size 'w'.\n",
    "    '''  \n",
    "    #(1) Terms to graph\n",
    "    terms = [term for term in terms if term]   #remove the padding zero\n",
    "        \n",
    "    v_nb = len(vocab)\n",
    "    A = np.zeros((v_nb,v_nb))  #adjacency matrix\n",
    "    D = np.zeros((v_nb,v_nb))\n",
    "    for term in terms:\n",
    "        D[vocab[term],vocab[term]] = 1\n",
    "    # create initial complete graph (first w terms)\n",
    "    terms_temp = terms[0:w]\n",
    "    indexes = list(itertools.combinations(range(w), r=2))\n",
    "    # add edges\n",
    "    edges = []\n",
    "    for my_tuple in indexes:\n",
    "        ii = vocab[terms_temp[my_tuple[0]]]    # index of vertice\n",
    "        jj = vocab[terms_temp[my_tuple[1]]]\n",
    "        if ii != jj:\n",
    "            A[ii, jj] = 1\n",
    "            A[jj, ii] = 1\n",
    "            edges.append((ii,jj))\n",
    "    # then iterate over the remaining terms\n",
    "    for i in range(w, len(terms)):\n",
    "        considered_term = terms[i] # term to consider\n",
    "        terms_temp = terms[(i-w+1):(i+1)] # all terms within sliding window\n",
    "        \n",
    "        # edges to try\n",
    "        candidate_edges = []\n",
    "        for p in range(w-1):\n",
    "            candidate_edges.append((terms_temp[p],considered_term))\n",
    "        \n",
    "        for try_edge in candidate_edges:          \n",
    "            if try_edge[1] != try_edge[0]:\n",
    "            # if not self-edge\n",
    "                ii = vocab[try_edge[0]]\n",
    "                jj = vocab[try_edge[1]]\n",
    "                A[ii, jj] = 1\n",
    "                A[jj, ii] = 1\n",
    "                edges.append((ii,jj))\n",
    "    \n",
    "    #print A, np.sum(A)\n",
    "    #print D\n",
    "    \n",
    "    # create empty graph\n",
    "    g = nx.Graph()\n",
    "    # add vertices\n",
    "    g.add_nodes_from(sorted(vocab.values()))\n",
    "    g.add_edges_from(edges)\n",
    "    \n",
    "    return(g)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
