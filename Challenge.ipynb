{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from keras.layers import GlobalAveragePooling1D, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = {}\n",
    "pairs_train = []\n",
    "pairs_test = []\n",
    "y_train = []\n",
    "with codecs.open('train.csv','r', 'UTF-8') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        if l[1] not in texts:\n",
    "            texts[l[1]] = l[3]\n",
    "        if l[2] not in texts:\n",
    "            texts[l[2]] = l[4]\n",
    "        pairs_train.append([l[1],l[2]])\n",
    "        y_train.append(int(l[5][:-1])) # [:-1] is just to remove formatting at the end\n",
    "\n",
    "with codecs.open('test.csv','r', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        if l[1] not in texts:\n",
    "            texts[l[1]] = l[3]\n",
    "        if l[2] not in texts:\n",
    "            texts[l[2]] = l[4][:-1]\n",
    "        pairs_test.append([l[1], l[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58940"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)  #type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 10, 1, 22, 11, 1, 16, 1154]\n",
      "73 max_size\n",
      "[   2   10    1   22   11    1   16 1154    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "docs = texts.values()\n",
    "# prepare tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "encoded_docs = tokenizer.texts_to_sequences(docs)\n",
    "#print (docs[0])\n",
    "print (encoded_docs[0])\n",
    "#tokenizer.word_index\n",
    "\n",
    "# pad documents to the same length\n",
    "max_size = max([len(t) for t in encoded_docs])\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_size, padding='post')\n",
    "print (max_size, 'max_size')\n",
    "print (padded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(t) for t in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(t) for t in encoded_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'are', 'the', 'some', 'of', 'the', 'best', 'novels']\n",
      "599985 words\n",
      "20354 unique words\n",
      "20354 tokenizer.word_index\n"
     ]
    }
   ],
   "source": [
    "index_to_word = dict((v,k) for k, v in tokenizer.word_index.items())\n",
    "# stpwds = [index_to_word[idx] for idx in range(1,stpwd_thd)]\n",
    "# print('stopwords are:',stpwds)\n",
    "x_full_words = [[index_to_word[idx] for idx in rev if idx!=0] for rev in encoded_docs]\n",
    "all_words = [word for rev in x_full_words for word in rev]\n",
    "print (x_full_words[0])\n",
    "print (len(all_words),'words')\n",
    "print (len(list(set(all_words))),'unique words')\n",
    "print (len(tokenizer.word_index), 'tokenizer.word_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_token is used for model fit\n",
    "texts_token = dict((ID,int(i)) for i,(ID,s) in enumerate(texts.items()))\n",
    "#padded_docs[texts_token[ID]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 word_vector_dim\n"
     ]
    }
   ],
   "source": [
    "# Use pre-trained\n",
    "# initialize word vectors\n",
    "word_vector_dim = int(3e2)\n",
    "print (word_vector_dim, 'word_vector_dim')\n",
    "word_vectors = Word2Vec(size=word_vector_dim, min_count=1)\n",
    "# create entries for the words in our vocabulary\n",
    "word_vectors.build_vocab(x_full_words)\n",
    "# sanity check\n",
    "##assert(len(list(set(all_words))) == len(word_vectors.wv.vocab))\n",
    "word_vectors.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lixin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of vocab words w/o a Google News entry: 5352\n"
     ]
    }
   ],
   "source": [
    "norms = [np.linalg.norm(word_vectors[word]) for word in list(word_vectors.wv.vocab)] # in Python 2.7: word_vectors.wv.vocab.keys()\n",
    "idxs_zero_norms = [idx for idx, norm in enumerate(norms) if norm<0.05]\n",
    "no_entry_words = [list(word_vectors.wv.vocab)[idx] for idx in idxs_zero_norms]\n",
    "print('# of vocab words w/o a Google News entry:',len(no_entry_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20354 unique words\n",
      "20354 word_vectors.wv.vocab\n",
      "20354 tokenizer.word_index\n"
     ]
    }
   ],
   "source": [
    "print (len(list(set(all_words))),'unique words')\n",
    "print (len(word_vectors.wv.vocab), 'word_vectors.wv.vocab')\n",
    "print (len(tokenizer.word_index), 'tokenizer.word_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20355 max_features\n",
      "embeddings created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lixin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# create numpy array of embeddings \n",
    "max_features = len(word_vectors.wv.vocab)+1  # nb of unique words\n",
    "print (max_features, 'max_features')\n",
    "embeddings = np.zeros((max_features, word_vector_dim))\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    embeddings[idx,] = word_vectors[word]\n",
    "print('embeddings created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_conv1D_(emb_matrix):\n",
    "    \n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=max_size,\n",
    "        trainable=False\n",
    "    )\n",
    "    print (emb_matrix.shape, 'emb_matrix shape')\n",
    "    \n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(max_size,))\n",
    "    seq2 = Input(shape=(max_size,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    conv1b = conv1(emb2)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "    conv2b = conv2(emb2)\n",
    "    glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    conv3b = conv3(emb2)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "    conv4b = conv4(emb2)\n",
    "    glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    conv5b = conv5(emb2)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "    conv6b = conv6(emb2)\n",
    "    glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "    #print (glob1a.shape,glob2a.shape,glob5a.shape)\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "    # We take the explicit absolute difference between the two sentences\n",
    "    # Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "\n",
    "    # Add the magic features\n",
    "    ###magic_input = Input(shape=(5,))\n",
    "    ###magic_dense = BatchNormalization()(magic_input)\n",
    "    ###magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "    # Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "    # nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "    #distance_input = Input(shape=(20,))\n",
    "    #distance_dense = BatchNormalization()(distance_input)\n",
    "    #distance_dense = Dense(128, activation='relu')(distance_dense)\n",
    "\n",
    "    # Merge the Magic and distance features with the difference layer\n",
    "    #merge = concatenate([diff, mul, magic_dense, distance_dense])\n",
    "    #merge = concatenate([diff, mul, magic_dense])\n",
    "    merge = concatenate([diff, mul])\n",
    "\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[seq1, seq2], outputs=pred)\n",
    "    #model = Model(inputs=[seq1, seq2, magic_input], outputs=pred)\n",
    "    #model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = len(pairs_train)  ####padded_docs[texts_token[ID]]\n",
    "X_train1 = []\n",
    "X_train2 = []\n",
    "for i in range(len(pairs_train)):\n",
    "    q1 = pairs_train[i][0]\n",
    "    q2 = pairs_train[i][1]\n",
    "    X_train1 = X_train1 + [padded_docs[texts_token[q1]]]\n",
    "    X_train2 = X_train2 + [padded_docs[texts_token[q2]]]\n",
    "\n",
    "N_test = len(pairs_test)\n",
    "X_test1 = []\n",
    "X_test2 = []\n",
    "for i in range(len(pairs_test)):\n",
    "    q1 = pairs_test[i][0]\n",
    "    q2 = pairs_test[i][1]\n",
    "    X_test1 = X_test1 + [padded_docs[texts_token[q1]]]\n",
    "    X_test2 = X_test2 + [padded_docs[texts_token[q2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.array(X_train1).shape, np.array(X_train2).shape, max_size)\n",
    "print (np.array(X_test1).shape, np.array(X_test2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20179, 73)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.array(X_test1), np.array(X_test2)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsFreq = Counter(all_words)\n",
    "#print (wordsFreq)\n",
    "#wordsFreq['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20354"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordsFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 28161, 1: 51939})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.42217961,  0.77109686])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40049.99981154"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.77109686*51939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40049.99999721"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.42217961*28161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20355, 300) emb_matrix shape\n",
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/5\n",
      "72090/72090 [==============================] - 237s 3ms/step - loss: 0.5542 - acc: 0.7267 - val_loss: 0.5691 - val_acc: 0.7782\n",
      "Epoch 2/5\n",
      "72090/72090 [==============================] - 300s 4ms/step - loss: 0.3995 - acc: 0.8217 - val_loss: 0.3911 - val_acc: 0.8221\n",
      "Epoch 3/5\n",
      "72090/72090 [==============================] - 227s 3ms/step - loss: 0.3176 - acc: 0.8658 - val_loss: 0.3617 - val_acc: 0.8536\n",
      "Epoch 4/5\n",
      "72090/72090 [==============================] - 258s 4ms/step - loss: 0.2564 - acc: 0.8940 - val_loss: 0.3653 - val_acc: 0.8370\n",
      "Epoch 5/5\n",
      "72090/72090 [==============================] - 230s 3ms/step - loss: 0.2156 - acc: 0.9123 - val_loss: 0.3498 - val_acc: 0.8615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5e03a828>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [np.array(X_train1),np.array(X_train2)]\n",
    "model = model_conv1D_(embeddings)\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size = 128,\n",
    "          epochs = 5,         #3\n",
    "          verbose = 1,\n",
    "          validation_split = 0.1,\n",
    "          class_weight={0: class_weights[0], 1: class_weights[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/3\n",
      "72090/72090 [==============================] - 232s 3ms/step - loss: 0.5518 - acc: 0.7286 - val_loss: 0.5485 - val_acc: 0.8076\n",
      "Epoch 2/3\n",
      "72090/72090 [==============================] - 235s 3ms/step - loss: 0.3980 - acc: 0.8242 - val_loss: 0.3766 - val_acc: 0.8449\n",
      "Epoch 3/3\n",
      "72090/72090 [==============================] - 232s 3ms/step - loss: 0.3151 - acc: 0.8669 - val_loss: 0.3652 - val_acc: 0.8424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3b43f940>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [np.array(X_train1),np.array(X_train2)]\n",
    "model = model_conv1D_(embeddings)\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size = 128,\n",
    "          epochs = 3,         #3\n",
    "          verbose = 1,\n",
    "          validation_split = 0.1,\n",
    "          class_weight={0: class_weights[0], 1: class_weights[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20355, 300) emb_matrix shape\n",
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/3\n",
      "72090/72090 [==============================] - 235s 3ms/step - loss: 0.5165 - acc: 0.7559 - val_loss: 0.5088 - val_acc: 0.7494\n",
      "Epoch 2/3\n",
      "72090/72090 [==============================] - 242s 3ms/step - loss: 0.3712 - acc: 0.8376 - val_loss: 0.3793 - val_acc: 0.8323\n",
      "Epoch 3/3\n",
      "72090/72090 [==============================] - 516s 7ms/step - loss: 0.2933 - acc: 0.8761 - val_loss: 0.4003 - val_acc: 0.8280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4ff2edd8>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [np.array(X_train1),np.array(X_train2)]\n",
    "model = model_conv1D_(embeddings)\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size = 128,\n",
    "          epochs = 3,         #3\n",
    "          verbose = 1,\n",
    "          validation_split = 0.1)\n",
    "          #, class_weight={0: 1, 1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20355, 300) emb_matrix shape\n",
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/6\n",
      "72090/72090 [==============================] - 233s 3ms/step - loss: 0.5192 - acc: 0.7558 - val_loss: 0.5551 - val_acc: 0.6823\n",
      "Epoch 2/6\n",
      "72090/72090 [==============================] - 226s 3ms/step - loss: 0.3744 - acc: 0.8360 - val_loss: 0.3867 - val_acc: 0.8270\n",
      "Epoch 3/6\n",
      "72090/72090 [==============================] - 227s 3ms/step - loss: 0.2980 - acc: 0.8743 - val_loss: 0.4127 - val_acc: 0.8288\n",
      "Epoch 4/6\n",
      "72090/72090 [==============================] - 228s 3ms/step - loss: 0.2400 - acc: 0.9021 - val_loss: 0.3435 - val_acc: 0.8591\n",
      "Epoch 5/6\n",
      "72090/72090 [==============================] - 227s 3ms/step - loss: 0.2027 - acc: 0.9177 - val_loss: 0.3225 - val_acc: 0.8693\n",
      "Epoch 6/6\n",
      "72090/72090 [==============================] - 228s 3ms/step - loss: 0.1721 - acc: 0.9311 - val_loss: 0.3472 - val_acc: 0.8644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5b468d68>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [np.array(X_train1),np.array(X_train2)]\n",
    "model = model_conv1D_(embeddings)\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size = 128,\n",
    "          epochs = 6,         #3\n",
    "          verbose = 1,\n",
    "          validation_split = 0.1)\n",
    "          #, class_weight={0: 1, 1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = [np.array(X_test1),np.array(X_test2)]\n",
    "y_pred = model.predict(X_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98650408],\n",
       "       [ 0.99972719],\n",
       "       [ 0.99992728],\n",
       "       [ 0.99705744],\n",
       "       [ 0.99415094]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        f.write(str(i)+','+str(y_pred[i][0])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids2ind = {} # will contain the row idx of each unique text in the TFIDF matrix \n",
    "for qid in texts:\n",
    "    ids2ind[qid] = len(ids2ind)\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "A = vec.fit_transform(texts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = len(pairs_train)\n",
    "X_train = np.zeros((N_train, 3))\n",
    "for i in range(len(pairs_train)):\n",
    "    q1 = pairs_train[i][0]\n",
    "    q2 = pairs_train[i][1]\n",
    "    X_train[i,0] = cosine_similarity(A[ids2ind[q1],:], A[ids2ind[q2],:])\n",
    "    X_train[i,1] = len(texts[q1].split()) + len(texts[q2].split())\n",
    "    X_train[i,2] = abs(len(texts[q1].split()) - len(texts[q2].split()))\n",
    "\n",
    "N_test = len(pairs_test)\n",
    "X_test = np.zeros((N_test, 3))\n",
    "for i in range(len(pairs_test)):\n",
    "    q1 = pairs_test[i][0]\n",
    "    q2 = pairs_test[i][1]\n",
    "    X_test[i,0] = cosine_similarity(A[ids2ind[q1],:], A[ids2ind[q2],:])\n",
    "    X_test[i,1] = len(texts[q1].split()) + len(texts[q2].split())\n",
    "    X_test[i,2] = abs(len(texts[q1].split()) - len(texts[q2].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=3, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80100, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = len(pairs_train)\n",
    "X_train1 = np.zeros((N_train, 4))\n",
    "X_train1[:,:3] = X_train\n",
    "for i in range(len(pairs_train)):\n",
    "    q1 = pairs_train[i][0]\n",
    "    q2 = pairs_train[i][1]\n",
    "    X_train1[i,3] = symmetric_sentence_similarity(texts[q1], texts[q2])\n",
    "\n",
    "N_test = len(pairs_test)\n",
    "X_test1 = np.zeros((N_test, 4))\n",
    "X_test1[:,:3] = X_test\n",
    "for i in range(len(pairs_test)):\n",
    "    q1 = pairs_test[i][0]\n",
    "    q2 = pairs_test[i][1]\n",
    "    X_test1[i,3] = symmetric_sentence_similarity(texts[q1], texts[q2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train1[:,3])):\n",
    "    if X_train1[i,3]<0:\n",
    "        X_train1[i,3] = X_train1[i,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_train = len(pairs_train)\n",
    "X_train2 = np.zeros((N_train, 5))\n",
    "X_train2[:,:4] = X_train1\n",
    "for i in range(len(pairs_train)):\n",
    "    q1 = pairs_train[i][0]\n",
    "    q2 = pairs_train[i][1]\n",
    "    X_train2[i,4] = similarity(texts[q1], texts[q2], True)\n",
    "\n",
    "N_test = len(pairs_test)\n",
    "X_test2 = np.zeros((N_test, 5))\n",
    "X_test2[:,:4] = X_test1\n",
    "for i in range(len(pairs_test)):\n",
    "    q1 = pairs_test[i][0]\n",
    "    q2 = pairs_test[i][1]\n",
    "    X_test2[i,4] = similarity(texts[q1], texts[q2], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 4), (0, 4), (0, 4))\n",
      "(80100, 53667, 26433)\n"
     ]
    }
   ],
   "source": [
    "# Split training data\n",
    "N_train = len(pairs_train)\n",
    "train_size = int(N_train * 0.67)\n",
    "X_trainTrain = X_train[:train_size, :]\n",
    "X_trainTest = X_train[train_size:, :]\n",
    "\n",
    "y_trainTrain = y_train[:train_size]\n",
    "y_trainTest = y_train[train_size:]\n",
    "print (X_train1.shape, X_trainTrain.shape, X_trainTest.shape)\n",
    "print (len(y_train), len(y_trainTrain), len(y_trainTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = MLPClassifier()\n",
    "#clf1 = RandomForestClassifier(n_estimators=500, max_depth=3, n_jobs=-1)\n",
    "clf1.fit(X_trainTrain, y_trainTrain)   # X_trainTrain[:,(0,3)]\n",
    "y_pred = clf1.predict_proba(X_trainTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55977646975724693"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "#y_true = [0, 0, 1, 1]\n",
    "#y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n",
    "y_true = y_trainTest\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qjqj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qjqj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of the test data, then output to a CSV file\n",
    "clf1 = MLPClassifier()\n",
    "clf1.fit(X_train1, y_train)\n",
    "y_pred = clf1.predict_proba(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"submission_file.csv\", 'w') as f:\n",
    "    f.write(\"Id,Score\\n\")\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        f.write(str(i)+','+str(y_pred[i][1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train features\n",
    "with open(\"save_train.csv\", 'w') as f:\n",
    "    f.write(\"TFIdf, len, len1, Wordnet\\n\")\n",
    "    for i in range(N_train):\n",
    "        f.write(str(X_train1[i,0])+','+str(X_train1[i,1])+','+str(X_train1[i,2])+','+str(X_train1[i,3])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.58723412,  22.        ,   6.        ,   0.88541667],\n",
       "       [  0.44438048,  21.        ,   3.        ,   0.69305556],\n",
       "       [  0.47480977,  26.        ,   6.        ,   0.890625  ],\n",
       "       ..., \n",
       "       [  0.62334832,  13.        ,   1.        ,   0.70634921],\n",
       "       [  0.71035487,  38.        ,   2.        ,   0.95833333],\n",
       "       [  0.6320477 ,  18.        ,   0.        ,   0.46666667]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('save_train.csv')\n",
    "df_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = df_train.values"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
